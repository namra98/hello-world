{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/namra98/hello-world/blob/master/cnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "2YIpNi-AKc3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "outputId": "57fd8a02-9f78-42dd-aa9e-b31908db138e"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "def run_cnn():\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "    learning_rate = 0.0001\n",
        "    epochs = 10\n",
        "    batch_size = 50\n",
        "\n",
        "    x = tf.placeholder(tf.float32, [None, 784])\n",
        "    x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
        "    y = tf.placeholder(tf.float32, [None, 10])\n",
        "    layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], 'layer1')\n",
        "    layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], 'layer2')\n",
        "    flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
        "    # fully connected\n",
        "    wd1 = tf.Variable(tf.truncated_normal(\n",
        "        [7 * 7 * 64, 1000], stddev=0.3), name='wd1')\n",
        "    bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
        "    dense_layer_1 = tf.matmul(flattened, wd1) + bd1\n",
        "    dense_layer_1 = tf.nn.relu(dense_layer_1)\n",
        "\n",
        "    wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
        "    bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
        "    dense_layer_2 = tf.matmul(dense_layer_1, wd2) + bd2\n",
        "    y_ = tf.nn.softmax(dense_layer_2)\n",
        "\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer_2, labels=y))\n",
        "\n",
        "    optimiser = tf.train.AdamOptimizer(\n",
        "        learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "    correct_prdiction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prdiction, tf.float32))\n",
        "\n",
        "    init_op = tf.global_variables_initializer()\n",
        "\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    merged = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter('C:\\\\Users\\\\NAMRA\\\\Desktop\\\\GAN')\n",
        "    with tf.Session() as sess:\n",
        "        # initialise the variables\n",
        "        sess.run(init_op)\n",
        "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            avg_cost = 0\n",
        "            for i in range(total_batch):\n",
        "                batch_x, batch_y = mnist.train.next_batch(\n",
        "                    batch_size=batch_size)\n",
        "                _, c = sess.run([optimiser, cross_entropy],\n",
        "                                feed_dict={x: batch_x, y: batch_y})\n",
        "                avg_cost += c / total_batch\n",
        "            test_acc = sess.run(accuracy, feed_dict={\n",
        "                                x: mnist.test.images, y: mnist.test.labels})\n",
        "            print(\"Epoch:\", (epoch + 1), \"cost =\",\n",
        "                  \"{:.3f}\".format(avg_cost), \" test accuracy: {:.3f}\".format(test_acc))\n",
        "            summary = sess.run(merged, feed_dict={\n",
        "                               x: mnist.test.images, y: mnist.test.labels})\n",
        "            writer.add_summary(summary, epoch)\n",
        "\n",
        "        print(\"\\nTraining complete!\")\n",
        "        writer.add_graph(sess.graph)\n",
        "        print(sess.run(accuracy, feed_dict={\n",
        "              x: mnist.test.images, y: mnist.test.labels}))\n",
        "\n",
        "\n",
        "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
        "    conv_filt_shape = [filter_shape[0], filter_shape[1],\n",
        "                       num_input_channels, num_filters]\n",
        "    weights = tf.Variable(tf.truncated_normal(\n",
        "        conv_filt_shape, stddev=0.3), name=name + '_W')\n",
        "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
        "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
        "    out_layer += bias\n",
        "    out_layer = tf.nn.relu(out_layer)\n",
        "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
        "    strides = [1, 2, 2, 1]\n",
        "    out_layer = tf.nn.max_pool(\n",
        "        out_layer, ksize=ksize, strides=strides, padding='SAME')\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_cnn()\n",
        "\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#\t sess.run(init_op)\n",
        "#\t total_batch = int(len(mnist.train.lables) / batch_size)\n",
        "#\t for epoch in range(epochs):\n",
        "#\t\t avg_cost = 0\n",
        "#\t\t for i in range(total_batch):\n",
        "#\t\t\t batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "#\t\t\t _, c = sess.run([optimiser, cross_entropy],\n",
        "#\t\t\t\t\t\t\t feed_dict={x: batch_x, y: batch_y})\n",
        "#\t\t\t avg_cost += c / total_batch\n",
        "#\t\t test_acc = sess.run(accuracy, feed_dict={\n",
        "#\t\t\t\t\t\t\t x: mnist.test.images, y: mnist.test.lables})\n",
        "#\t\t print(\"Epoch:\", (epoch + 1), \"cost = \", \"{:3f}\".format(avg_cost), \"test accurecy : {:3f}\".format(test_acc))\n",
        "#\t print(\"\\nTraining Completed\")\n",
        "#\t print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-3f9a88c023e6>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From <ipython-input-1-3f9a88c023e6>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 1 cost = 1.830  test accuracy: 0.918\n",
            "Epoch: 2 cost = 0.498  test accuracy: 0.923\n",
            "Epoch: 3 cost = 0.340  test accuracy: 0.958\n",
            "Epoch: 4 cost = 0.259  test accuracy: 0.943\n",
            "Epoch: 5 cost = 0.208  test accuracy: 0.962\n",
            "Epoch: 6 cost = 0.160  test accuracy: 0.968\n",
            "Epoch: 7 cost = 0.132  test accuracy: 0.971\n",
            "Epoch: 8 cost = 0.103  test accuracy: 0.975\n",
            "Epoch: 9 cost = 0.088  test accuracy: 0.973\n",
            "Epoch: 10 cost = 0.066  test accuracy: 0.976\n",
            "\n",
            "Training complete!\n",
            "0.9762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VO_CdG6_iIyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0nfrgB9XyZ_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZLpA1cTuqF7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}